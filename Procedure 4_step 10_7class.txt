import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from PIL import Image

# Define dataset path
data_dir = r"C:\7class"

# Define image transformations, including basic data augmentation
transform_default = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self, samples, labels, transform=None):
        self.samples = samples
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path = self.samples[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.labels[idx]

        # Apply default transform (fine-tuning augmentation can be added here as needed)
        if self.transform:
            image = self.transform(image)

        return image, label

# Load all image data from BS1 to BS7 folders
samples = []
labels = []
valid_folders = ['BS1', 'BS2', 'BS3', 'BS4', 'BS5', 'BS6', 'BS7']
class_to_idx = {cls_name: idx for idx, cls_name in enumerate(valid_folders)}

for class_name in valid_folders:
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            if os.path.isfile(img_path):
                samples.append(img_path)
                labels.append(class_to_idx[class_name])

# Ensure all labels are within the [0, 6] range
for idx, label in enumerate(labels):
    if label < 0 or label >= 7:
        print(f"Found an invalid label at index {idx}: {label}")
        labels[idx] = 0  # Adjust label handling logic as necessary

# Split the data into training and validation sets
train_samples, val_samples, train_labels, val_labels = train_test_split(
    samples, labels, test_size=0.2, stratify=labels, random_state=42
)

# Create data loaders for training and validation sets
train_dataset = CustomDataset(train_samples, train_labels, transform=transform_default)
val_dataset = CustomDataset(val_samples, val_labels, transform=transform_default)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define the model
model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
num_ftrs = model.classifier[1].in_features  # Get the number of input features for the last fully connected layer

# Modify the last layer for seven-class classification
model.classifier[1] = nn.Linear(num_ftrs, 7)

# Move the model to the appropriate device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Define the loss function and optimizer
# Note: fine-tuning hyperparameters are hidden, this can be adjusted as needed
# Example: you can add specific class weights, change learning rate, etc.
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Fine-tuned learning rate is hidden here

# Ensure the model save directory exists
model_save_dir = r"C:\7class\saved_models"
os.makedirs(model_save_dir, exist_ok=True)

# Training the model
num_epochs = 50  # Number of epochs can be adjusted based on fine-tuning
best_val_acc = 0.0
early_stopping_patience = 10
no_improvement_epochs = 0

train_acc_history = []
val_acc_history = []
best_test_preds = []
best_test_labels = []

for epoch in range(num_epochs):
    # Training phase
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_acc = correct / total
    train_acc_history.append(train_acc)

    # Validation phase
    model.eval()
    val_correct = 0
    val_total = 0
    val_preds = []
    val_labels_list = []

    with torch.no_grad():
        for val_inputs, val_labels in val_loader:
            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)

            val_outputs = model(val_inputs)
            _, val_predicted = torch.max(val_outputs, 1)
            val_total += val_labels.size(0)
            val_correct += (val_predicted == val_labels).sum().item()

            val_preds.extend(val_predicted.cpu().numpy())
            val_labels_list.extend(val_labels.cpu().numpy())

    val_acc = val_correct / val_total
    val_acc_history.append(val_acc)

    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

    # Save the best model and prediction results
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_test_preds = val_preds
        best_test_labels = val_labels_list
        torch.save(model, os.path.join(model_save_dir, '7class.pth'))  # Save the complete model
        print(f"Best model saved: epoch {epoch + 1}")
        no_improvement_epochs = 0
    else:
        no_improvement_epochs += 1

    # Early stopping mechanism
    if no_improvement_epochs >= early_stopping_patience:
        print(f"Early stopping triggered at epoch {epoch + 1}")
        break

# Load the best model for testing
model = torch.load(os.path.join(model_save_dir, '7class.pth'))

# Plot the graph of EPOCH and train, test accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(train_acc_history) + 1), train_acc_history, label='Train Accuracy')
plt.plot(range(1, len(val_acc_history) + 1), val_acc_history, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train and Validation Accuracy over Epochs')
plt.legend()
plt.show()

# Compute the classification report and confusion matrix for the predictions of the best model
cr = classification_report(best_test_labels, best_test_preds, target_names=valid_folders, zero_division=1)
print("Classification Report:\n", cr)

# Plot the confusion matrix
cm = confusion_matrix(best_test_labels, best_test_preds)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=valid_folders,
            yticklabels=valid_folders)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for BS1 to BS7 (Best Epoch)')
plt.show()
