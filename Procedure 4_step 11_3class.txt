import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from PIL import Image

# Define dataset path
data_dir = r"C:\3class"

# Define image transformations, including data augmentation
transform_default = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Data augmentation specifically for Class1 (kept for demonstration purposes)
transform_class1 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(30),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self, samples, labels, transform=None):
        self.samples = samples
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path = self.samples[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.labels[idx]

        # Choose different transform based on the class
        if self.transform:
            if label == 0:  # Class1 specific transformation
                image = transform_class1(image)
            else:
                image = self.transform(image)

        return image, label

# Load all image data from Class1 to Class3 folders
samples = []
labels = []
valid_folders = ['Class1', 'Class2', 'Class3']
class_to_idx = {cls_name: idx for idx, cls_name in enumerate(valid_folders)}

for class_name in valid_folders:
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            if os.path.isfile(img_path):
                samples.append(img_path)
                labels.append(class_to_idx[class_name])

# Split the data into training and validation sets
train_samples, val_samples, train_labels, val_labels = train_test_split(
    samples, labels, test_size=0.2, stratify=labels, random_state=42
)

# Create data loaders for the training and validation sets
train_dataset = CustomDataset(train_samples, train_labels, transform=transform_default)
val_dataset = CustomDataset(val_samples, val_labels, transform=transform_default)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define the model
model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
num_ftrs = model.classifier[1].in_features  # Get the number of input features for the last fully connected layer

# Modify the last layer for 3-class classification
model.classifier[1] = nn.Linear(num_ftrs, 3)

# Move the model to the appropriate device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Define the loss function and optimizer
# The class weights can be adjusted based on dataset balancing
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  # Learning rate can be adjusted for tuning

# Training setup
num_epochs = 20  # Reduced epochs for quick demo, can be increased for full fine-tuning
best_val_acc = 0.0
early_stopping_patience = 5  # Early stopping patience set for quick convergence
no_improvement_epochs = 0

train_acc_history = []
val_acc_history = []
best_test_preds = []
best_test_labels = []

# Ensure the model save directory exists
model_save_dir = r"C:\3class\saved_models"
os.makedirs(model_save_dir, exist_ok=True)

for epoch in range(num_epochs):
    # Training phase
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_acc = correct / total
    train_acc_history.append(train_acc)

    # Validation phase
    model.eval()
    val_correct = 0
    val_total = 0
    val_preds = []
    val_labels_list = []

    with torch.no_grad():
        for val_inputs, val_labels in val_loader:
            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)

            val_outputs = model(val_inputs)
            _, val_predicted = torch.max(val_outputs, 1)
            val_total += val_labels.size(0)
            val_correct += (val_predicted == val_labels).sum().item()

            val_preds.extend(val_predicted.cpu().numpy())
            val_labels_list.extend(val_labels.cpu().numpy())

    val_acc = val_correct / val_total
    val_acc_history.append(val_acc)

    print(f'Epoch {epoch + 1}/{num_epochs}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

    # Save the best complete model (including architecture and weights)
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model, os.path.join(model_save_dir, '3class.pth'))  # Save complete model
        print(f"Best model saved: epoch {epoch + 1}")
        no_improvement_epochs = 0
    else:
        no_improvement_epochs += 1

    # Early stopping
    if no_improvement_epochs >= early_stopping_patience:
        print(f"Early stopping at epoch {epoch + 1}")
        break

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(train_acc_history) + 1), train_acc_history, label='Train Accuracy')
plt.plot(range(1, len(val_acc_history) + 1), val_acc_history, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train and Validation Accuracy over Epochs')
plt.legend()
plt.show()

# Load the best complete model for testing
model = torch.load(os.path.join(model_save_dir, '3class.pth'))
model.eval()

# Confusion Matrix and Classification Report
cr = classification_report(val_labels_list, val_preds, target_names=valid_folders, zero_division=1)
print("Classification Report:\n", cr)

cm = confusion_matrix(val_labels_list, val_preds)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=valid_folders, yticklabels=valid_folders)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
